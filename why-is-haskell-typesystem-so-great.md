# What exactly makes the Haskell type system so revered (vs say, Java)?

[Original source here](https://softwareengineering.stackexchange.com/questions/279316/what-exactly-makes-the-haskell-type-system-so-revered-vs-say-java)

<div class="post-text" itemprop="text">
        <p>Here's an unordered list of type system features available in Haskell and either unavailable or less nice in Java (to my knowledge, which is admittedly weak w.r.t. Java)</p>

<ul>
<li><strong>Safety</strong>. Haskell's types have pretty good "type safety" properties. This is pretty specific, but it essentially means that values at some type cannot wantonly transform into another type. This is sometimes at odds with mutability (see OCaml's <em>value restriction</em>)</li>
<li><strong>Algebraic Data Types</strong>. Types in Haskell have essentially the same structure as high school mathematics. This is <em>outrageously simple</em> and consistent, yet, as it turns out, as powerful as you could possibly want. It's simply a great foundation for a type system.

<ul>
<li><strong>Datatype-generic programming</strong>. This is not the same as generic types (see <em>generalization</em>). Instead, due to the simplicity of the type structure as noted before it's relatively easy to write code which operates generically over that structure. Later I talk about how something like <code>Eq</code>uality might be auto-derived for a user-defined type by a Haskell compiler. Essentially the way that it does this is walk over the common, simple structure underlying any user-defined type and match it up between values—a very natural form of structural equality.</li>
</ul></li>
<li><strong>Mutually recursive types</strong>. This is just an essential component of writing non-trivial types.

<ul>
<li><strong>Nested types</strong>. This allows you to define recursive types over variables which recurse at different types. For instance, one type of balanced trees is <code>data Bt a = Here a | There (Bt (a, a))</code>. Think carefully about the valid values of <code>Bt a</code> and notice how that type works. It's tricky!</li>
</ul></li>
<li><strong>Generalization</strong>. This is almost too silly to not have in a type system (ahem, looking at you, Go). It's important to have notions of type variables and the ability to talk about code which is independent of the choice of that variable. Hindley Milner is a type system which is derived from System F. Haskell's type system is an elaboration of HM typing and System F is essentially the <em>hearth</em> of generalization. What I mean to say is that Haskell has a <em>very good</em> generalization story.</li>
<li><strong>Abstract types</strong>. Haskell's story here is not great but also not non-existent. It's possible to write types which have a public interface but a private implementation. This allows us to both admit changes to the implementation code at a later time and, importantly since it's the basis of all operation in Haskell, write "magic" types which have well-defined interfaces such as <code>IO</code>. Java probably actually has a nicer abstract type story, to be honest, but I don't think until Interfaces became more popular was that genuinely true.</li>
<li><strong>Parametricity</strong>. Haskell values do not have <em>any</em> universal operations. Java violates this with things like reference equality and hashing and even more flagrantly with coercions. What this means is that you get <em>free theorems</em> about types which allow you to know the meaning of an operation or value to a remarkable degree entirely from its type---certain types are such that there can only be a very small number of inhabitants.</li>
<li><strong>Higher-kinded types</strong> show up all the type when encoding trickier things. Functor/Applicative/Monad, Foldable/Traversable, the entire <code>mtl</code> effect typing system, generalized functor fixpoints. The list goes on and on. There are a lot of things which are best expressed at higher kinds and relatively few type systems even allow the user to talk about these things.</li>
<li><strong>Type classes</strong>. If you think of type systems as logics—which is useful—then you often are demanded to prove things. In many cases this is essentially line noise: there may be only one right answer and it's a waste of time and effort for the programmer to state this. Typeclasses are a way for Haskell to generate the proofs for you. In more concrete terms, this lets you solve simple "type equation systems" like "At which type are we intending to <code>(+)</code> things together? Oh, <code>Integer</code>, ok! Let's inline the right code now!". At more complex systems you might be establishing more interesting constraints.

<ul>
<li><strong>Constraint calculus</strong>. Constraints in Haskell—which are the mechanism for reaching into the typeclass prolog system—are structurally typed. This gives a <em>very simple</em> form of subtyping relationship which lets you assemble complex constraints from simpler ones. The entire <code>mtl</code> library is based on this idea.</li>
<li><strong>Deriving</strong>. In order to drive the <em>canonicity</em> of the typeclass system it's necessary to write a lot of often trivial code to describe the constraints user-defined types must instantiate. Do to the very normal structure of Haskell types, it is often possible to ask the compiler to do this boilerplate for you.</li>
<li><strong>Type class prolog</strong>. The Haskell type class solver—the system which is generating those "proofs" I referred to earlier—is essentially a crippled form of Prolog with nicer semantic properties. This means you can encode really hairy things in type prolog and expect them to be handled all at compile time. A good example might be solving for a proof that two heterogenous lists are equivalent if you forget about the order—they're equivalent heterogenous "sets".</li>
<li><strong>Multi-parameter type classes and functional dependencies</strong>. These are just massively useful refinements to base typeclass prolog. If you know Prolog, you can imagine how much the expressive power increases when you can write predicates of more than one variable.</li>
</ul></li>
<li><strong>Pretty good inference</strong>. Languages based on Hindley Milner type systems have pretty good inference. HM itself has <em>complete</em> inference which means that you never need to write a type variable. Haskell 98, the simplest form of Haskell, already throws that out in some very rare circumstances. Generally, modern Haskell has been an experiment in slowly reducing the space of complete inference while adding more power to HM and seeing when users complain. People very rarely complain—Haskell's inference is pretty good.</li>
<li><strong>Very, very, very weak subtyping only</strong>. I mentioned earlier that the constraint system from typeclass prolog has a notion of structural subtyping. <em>That is the only form of subtyping in Haskell</em>. Subtyping is terrible for reasoning and inference. It makes each of those problems significantly harder (a system of inequalities instead of a system of equalities). It's also really easy to misunderstand (Is subclassing the same as subtyping? Of course not! But people very frequently confuse that and many languages aid in that confusion! How did we end up here? I suppose nobody ever examines the LSP.)</li>
<li><ul>
<li><em>Note</em> recently (early 2017) Steven Dolan published his <a href="https://www.cl.cam.ac.uk/~sd601/thesis.pdf" rel="noreferrer">thesis</a> on <a href="https://www.cl.cam.ac.uk/~sd601/mlsub/" rel="noreferrer">MLsub</a>, a variant of ML and Hindley-Milner type inference which has a <em>very nice</em> subtyping story (<a href="https://www.cl.cam.ac.uk/~sd601/papers/mlsub-preprint.pdf" rel="noreferrer">see also</a>). This doesn't obviate what I've written above—most subtyping systems are broken and have bad inference—but does suggest that we just today may have discovered some promising ways to have complete inference and subtyping play together nicely. Now, to be totally clear, Java's notions of subtyping are in no way able to take advantage of Dolan's algorithms and systems. It requires a rethinking of what subtyping means.</li>
</ul></li>
<li><strong>Higher rank types</strong>. I talked about generalization earlier, but more than just mere generalization it's useful to be able to talk about types which have generalized variables <em>within them</em>. For instance, a mapping between higher order structures which is oblivious (see <em>parametricity</em>) to what those structures "contain" has a type like <code>(forall a. f a -&gt; g a)</code>. In straight HM you can write a function at this type, but with higher-rank types you demand such a function as an <em>argument</em> like so: <code>mapFree :: (forall a . f a -&gt; g a) -&gt; Free f -&gt; Free g</code>. Notice that the <code>a</code> variable is bound only within the argument. This means that the <em>definer</em> of the function <code>mapFree</code> gets to decide what <code>a</code> is instantiated at when they use it, not the user of <code>mapFree</code>.</li>
<li><strong>Existential types</strong>. While higher-rank types allow us to talk about universal quantification, existential types let us talk about existential quantification: the idea that there merely <em>exists</em> some unknown type satisfying some equations. This ends up being useful and to go on for longer about it would take a long while.</li>
<li><strong>Type families</strong>. Sometimes the typeclass mechanisms are inconvenient since we don't always think in Prolog. Type families let us write straight <em>functional</em> relationships between types.

<ul>
<li><strong>Closed type families</strong>. Type families are by default open which is annoying because it means that while you can extend them at any time you cannot "invert" them with any hope of success. This is because you cannot prove <em>injectiveness</em>, but with closed type families you can.</li>
</ul></li>
<li><p><strong>Kind indexed types and type promotion</strong>. I'm getting really exotic at this point, but these have practical use from time to time. If you'd like to write a type of handles which are either open or closed then you can do that very nicely. Notice in the following snippet that <code>State</code> is a very simple algebraic type which had its values promoted into the type-level as well. Then, subsequently, we can talk about <em>type constructors</em> like <code>Handle</code> as taking arguments at specific <em>kinds</em> like <code>State</code>. It's confusing to understand all the details, but also so very right.</p>

<pre class="lang-java prettyprint prettyprinted" style=""><code><span class="pln">data </span><span class="typ">State</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> </span><span class="typ">Open</span><span class="pln"> </span><span class="pun">|</span><span class="pln"> </span><span class="typ">Closed</span><span class="pln">

data </span><span class="typ">Handle</span><span class="pln"> </span><span class="pun">::</span><span class="pln"> </span><span class="typ">State</span><span class="pln"> </span><span class="pun">-&gt;</span><span class="pln"> </span><span class="pun">*</span><span class="pln"> </span><span class="pun">-&gt;</span><span class="pln"> </span><span class="pun">*</span><span class="pln"> where
  </span><span class="typ">OpenHandle</span><span class="pln"> </span><span class="pun">::</span><span class="pln"> </span><span class="pun">{-</span><span class="pln"> something </span><span class="pun">-}</span><span class="pln"> </span><span class="pun">-&gt;</span><span class="pln"> </span><span class="typ">Handle</span><span class="pln"> </span><span class="typ">Open</span><span class="pln"> a
  </span><span class="typ">ClosedHandle</span><span class="pln"> </span><span class="pun">::</span><span class="pln"> </span><span class="pun">{-</span><span class="pln"> something </span><span class="pun">-}</span><span class="pln"> </span><span class="pun">-&gt;</span><span class="pln"> </span><span class="typ">Handle</span><span class="pln"> </span><span class="typ">Closed</span><span class="pln"> a</span></code></pre></li>
<li><p><strong>Runtime type representations that work</strong>. Java is notorious for having type erasure and having that feature rain on some people's parades. Type erasure <em>is</em> the right way to go, however, as if you have a function <code>getRepr :: a -&gt; TypeRepr</code> then you at the very least violate parametricity. What's worse is that if that's a user-generated function which is used to trigger unsafe coercions at runtime... then you've got a <em>massive safety concern</em>. Haskell's <code>Typeable</code> system allows the creation of a safe <code>coerce :: (Typeable a, Typeable b) =&gt; a -&gt; Maybe b</code>. This system relies on <code>Typeable</code> being implemented in the compiler (and not userland) and also could not be given such nice semantics without Haskell's typeclass mechanism and the laws it is guaranteed to follow.</p></li>
</ul>

<p>More than just these however the value of Haskell's type system also relates to how the types describe the language. Here are a few features of Haskell which drive value through the type system.</p>

<ul>
<li><strong>Purity</strong>. Haskell allows no side effects for a very, very, very wide definition of "side effect". This forces you to put more information into types since types govern inputs and outputs and without side effects <em>everything</em> must be accounted for in the inputs and outputs.

<ul>
<li><strong>IO</strong>. Subsequently, Haskell needed a way to talk about side effects—since any real program must include some—so a combination of typeclasses, higher kinded types, and abstract types gave rise to the notion of using a particular, super-special type called <code>IO a</code> to represent side-effecting computations which result in values of type <code>a</code>. This is the foundation of a <em>very</em> nice effect system embedded inside of a pure language.</li>
</ul></li>
<li><strong>Lack of <code>null</code></strong>. Everyone knows that <code>null</code> is the billion dollar mistake of modern programming languages. Algebraic types, in particular the ability to just append a "does not exist" state onto types you have by transforming a type <code>A</code> into the type <code>Maybe A</code>, completely mitigate the problem of <code>null</code>.</li>
<li><strong>Polymorphic recursion</strong>. This lets you define recursive functions which generalize type variables despite using them at different types in each recursive call in their own generalization. This is difficult to talk about, but especially useful for talking about nested types. Look back to the <code>Bt a</code> type from before and try to write a function to compute its size: <code>size :: Bt a -&gt; Int</code>. It'll look a bit like <code>size (Here a) = 1</code> and <code>size (There bt) = 2 * size bt</code>. Operationally that isn't too complex, but notice that the recursive call to <code>size</code> in the last equation occurs <em>at a different type</em>, yet the overall definition has a nice generalized type <code>size :: Bt a -&gt; Int</code>. Note that this is a feature which breaks total inference, but if you provide a type signature then Haskell will allow it.</li>
</ul>

<p>I could keep going, but this list ought to get you started-and-then-some.</p>
</div>
